{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfDmOMvGynRY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "\n",
        "# Load YOLOv5 model\n",
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# Load video\n",
        "cap = cv2.VideoCapture('/content/Roadsafetyviolation_14.mp4')\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Initialize variables to track vehicle movement for illegal U-turn detection\n",
        "vehicle_positions = {}\n",
        "illegal_uturn_flagged = False\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        print(\"Error: Failed to read frame.\")\n",
        "        break\n",
        "\n",
        "    # Convert the image to RGB for pose detection\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image_rgb)\n",
        "\n",
        "    # YOLOv5 Object Detection\n",
        "    yolo_results = yolo_model(frame)\n",
        "    detections = yolo_results.pred[0]  # Get detections for the frame\n",
        "\n",
        "    # Draw bounding boxes from YOLO and track positions\n",
        "    for *box, conf, cls in detections:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        label = f\"{yolo_results.names[int(cls)]} {conf:.2f}\"\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Track vehicles for illegal U-turn detection\n",
        "        center_x = (x1 + x2) // 2\n",
        "        center_y = (y1 + y2) // 2\n",
        "        vehicle_id = f\"{int(cls)}-{center_x}-{center_y}\"  # Unique identifier based on class and position\n",
        "\n",
        "        # Update vehicle positions\n",
        "        if vehicle_id in vehicle_positions:\n",
        "            prev_x, prev_y = vehicle_positions[vehicle_id]\n",
        "            # Check if direction has reversed within a short span, indicating a potential illegal U-turn\n",
        "            if abs(center_x - prev_x) > 50 and not illegal_uturn_flagged:  # Threshold for direction change\n",
        "                cv2.putText(frame, \"Illegal U-turn Detected\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "                illegal_uturn_flagged = True\n",
        "        vehicle_positions[vehicle_id] = (center_x, center_y)\n",
        "\n",
        "    # Draw pose landmarks if detected for phone usage detection\n",
        "    if results.pose_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            frame,\n",
        "            results.pose_landmarks,\n",
        "            mp_pose.POSE_CONNECTIONS,\n",
        "            mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
        "            mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
        "        )\n",
        "\n",
        "        # Get key landmarks for phone usage detection\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "        left_hand = landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n",
        "        right_hand = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST]\n",
        "        left_ear = landmarks[mp_pose.PoseLandmark.LEFT_EAR]\n",
        "        right_ear = landmarks[mp_pose.PoseLandmark.RIGHT_EAR]\n",
        "\n",
        "        # Check if hands are near the head (phone usage detection)\n",
        "        def is_near_head(hand, ear, threshold=0.1):\n",
        "            distance = np.sqrt((hand.x - ear.x)**2 + (hand.y - ear.y)**2)\n",
        "            return distance < threshold\n",
        "\n",
        "        if is_near_head(left_hand, left_ear) or is_near_head(right_hand, right_ear):\n",
        "            cv2.putText(frame, \"Phone Usage Detected\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "    # Display the frame with object detection and pose landmarks\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Use a small delay between frames\n",
        "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "pose.close()"
      ]
    }
  ]
}